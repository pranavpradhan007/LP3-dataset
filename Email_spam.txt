








































import pandas as pd

df = pd.read_csv("emails.csv")

df.shape

df.head()

#input data
x = df.drop(["Email No.","Prediction"], axis = 1)

#output data
y = df["Prediction"]

x.shape

set(x.dtypes)

import seaborn as sns
sns.countplot(x = y)

y.value_counts()

# Feature scaling
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
x_scaled = scaler.fit_transform(x)

x_scaled

# Cross Validation
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, random_state=0, test_size=0.25)

x_scaled.shape

x_train.shape

x_test.shape

# import the class
from sklearn.neighbors import KNeighborsClassifier

# Create the object
knn = KNeighborsClassifier(n_neighbors=5)

# Train the algorithm
knn.fit(x_train, y_train)

# Predict on test data
y_pred = knn.predict(x_test)

# Import evaluation metrics
from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, classification_report

ConfusionMatrixDisplay.from_predictions(y_test, y_pred)

y_test.value_counts()

print("Accuracy for KNN: ",accuracy_score(y_test, y_pred))

print(classification_report(y_test, y_pred))

from sklearn.svm import SVC

svm = SVC(kernel='linear')

svm.fit(x_train, y_train)

y_pred = svm.predict(x_test)

print("Accuracy for SVM: ",accuracy_score(y_test, y_pred))

print(classification_report(y_test, y_pred))











Certainly! I'll explain the code line by line:

```python
import pandas as pd
```
This line imports the Pandas library, which is used for data manipulation and analysis.

```python
df = pd.read_csv("emails.csv")
```
It reads a CSV file named "emails.csv" into a Pandas DataFrame called `df`. This is typically done to load a dataset for further analysis.

```python
df.shape
```
This line outputs the dimensions of the DataFrame, which means it displays the number of rows and columns in the DataFrame.

```python
df.head()
```
This line shows the first few rows of the DataFrame, providing a quick look at the data.

```python
# Input data
x = df.drop(["Email No.", "Prediction"], axis=1)
```
Here, a new DataFrame `x` is created by dropping two columns, "Email No." and "Prediction," from the original DataFrame `df`. The `axis=1` argument indicates that the columns should be dropped.

```python
# Output data
y = df["Prediction"]
```
This line creates a Series `y` by selecting the "Prediction" column from the original DataFrame `df`. This is typically the target variable for a machine learning task.

```python
x.shape
```
This outputs the dimensions of the `x` DataFrame, showing the number of rows and columns.

```python
set(x.dtypes)
```
This line displays the unique data types of the columns in the `x` DataFrame. It helps you check the data types of the features.

```python
import seaborn as sns
sns.countplot(x=y)
```
Here, the Seaborn library is used to create a countplot of the "Prediction" column (`y`). This is a way to visualize the distribution of the target variable.

```python
y.value_counts()
```
This line shows the count of unique values in the "Prediction" column, which is helpful for understanding the class distribution in the target variable.

```python
# Feature scaling
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
x_scaled = scaler.fit_transform(x)
```
In this section, data scaling is performed. The MinMaxScaler from scikit-learn is imported, and a scaler object is created. The `fit_transform` method scales the features in `x` so that they are in the range [0, 1].

```python
# Cross Validation
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, random_state=0, test_size=0.25)
```
This part of the code splits the data into training and testing sets using the `train_test_split` function from scikit-learn. The `x_scaled` is the feature data, `y` is the target variable, and it's split into training and testing sets with a 75% train and 25% test ratio. The `random_state` argument ensures reproducibility.

```python
x_scaled.shape
```
Outputs the dimensions of the scaled feature data, showing the number of rows and columns.

```python
x_train.shape
```
Displays the dimensions of the training set.

```python
x_test.shape
```
Shows the dimensions of the testing set.

```python
# import the class
from sklearn.neighbors import KNeighborsClassifier
```
This line imports the KNeighborsClassifier class from scikit-learn, which is used to build a k-nearest neighbors classifier.

```python
# Create the object
knn = KNeighborsClassifier(n_neighbors=5)
```
An instance of the KNeighborsClassifier is created with `n_neighbors=5`, meaning it will use 5 neighbors to make predictions.

The code continues to fit, predict, and evaluate the K-Nearest Neighbors classifier and a Support Vector Machine (SVM) classifier, displaying accuracy and classification reports for both models. These steps are common in the machine learning workflow for binary or multi-class classification tasks.